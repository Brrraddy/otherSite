-- phpMyAdmin SQL Dump
-- version 5.0.1
-- https://www.phpmyadmin.net/
--
-- Хост: 127.0.0.1
-- Время создания: Май 31 2020 г., 21:01
-- Версия сервера: 10.4.11-MariaDB
-- Версия PHP: 7.4.2

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET AUTOCOMMIT = 0;
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- База данных: `test1base`
--

-- --------------------------------------------------------

--
-- Структура таблицы `news`
--

CREATE TABLE `news` (
  `id` int(11) UNSIGNED NOT NULL,
  `title` varchar(255) NOT NULL,
  `intro_text` text NOT NULL,
  `full_text` text NOT NULL
) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4;

--
-- Дамп данных таблицы `news`
--

INSERT INTO `news` (`id`, `title`, `intro_text`, `full_text`) VALUES
(1, 'Just first example', 'Marry, and will, my lord, to weep in such a one were prettiest;', 'Marry, and will, my lord, to weep in such a one were prettiest;\r\nYet now I was adopted heir\r\nOf the world\'s lamentable day,\r\nTo watch the next way with his father with his face?\r\nI say he look\'d on, if I must be content\r\nTo stay him from the fatal of our country\'s bliss.\r\nHis lordship pluck\'d from this sentence then for prey,\r\nAnd then let us twain, being the moon,\r\nwere she such a case as fills m\r\nGiven a character, or a sequence of characters, what is the most probable next character? This is the task we\'re training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\r\n\r\nSince RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?'),
(2, 'Other Example', 'Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character.', 'Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to retrieve and reuse the states from stateful RNN layer, you might want to build your model with Keras functional API or model subclassing. Please check Keras RNN guide for more details.Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?'),
(3, 'Other example new', 'Since RNNs maintain an internal state that depends on the previously seen ', 'Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\r\nGiven a character, or a sequence of characters, what is the most probable next character? This is the task we\'re training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.'),
(4, 'Hello dear ', 'In this article, I’ll briefly go over a simple way to code and train a text generation model in Python using Keras and Tensorflow. Our goal is to train a model to emulate the speaking style of the text it is trained on. In this', 'The first step to training any NLP model is the tokenization of words. Here we use the Keras Tokenizer, which does the following:\r\nRemoves punctuation\r\nSets all text to lower case\r\nSplits the words up into individual elements in a list, then assigns a unique integer to each word\r\nReplaces all instances of that word with the integer.\r\nTokenization is necessary for preparing data for embedding layer (see model architecture section below)\r\nThe first step to training any NLP model is the tokenization of words. Here we use the Keras Tokenizer, which does the following:\r\nRemoves punctuation\r\nSets all text to lower case\r\nSplits the words up into individual elements in a list, then assigns a unique integer to each word\r\nReplaces all instances of that word with the integer.\r\nTokenization is necessary for preparing data for embedding layer (see model architecture section below)'),
(5, 'This is other one', 'Tokenization is necessary for preparing data for embedding layer (see model architecture section below)', 'The first step to training any NLP model is the tokenization of words. Here we use the Keras Tokenizer, which does the following:\r\nRemoves punctuation\r\nSets all text to lower case\r\nSplits the words up into individual elements in a list, then assigns a unique integer to each word\r\nReplaces all instances of that word with the integer.\r\nTokenization is necessary for preparing data for embedding layer (see model architecture section below)\r\nThe first step to training any NLP model is the tokenization of words. Here we use the Keras Tokenizer, which does the following:\r\nRemoves punctuation\r\nSets all text to lower case\r\nSplits the words up into individual elements in a list, then assigns a unique integer to each word\r\nReplaces all instances of that word with the integer.\r\nTokenization is necessary for preparing data for embedding layer (see model architecture section below)\r\nThe first step to training any NLP model is the tokenization of words. Here we use the Keras Tokenizer, which does the following:\r\nRemoves punctuation\r\nSets all text to lower case\r\nSplits the words up into individual elements in a list, then assigns a unique integer to each word\r\nReplaces all instances of that word with the integer.\r\nTokenization is necessary for preparing data for embedding layer (see model architecture section below)\r\nThe first step to training any NLP model is the tokenization of words. Here we use the Keras Tokenizer, which does the following:\r\nRemoves punctuation\r\nSets all text to lower case\r\nSplits the words up into individual elements in a list, then assigns a unique integer to each word\r\nReplaces all instances of that word with the integer.\r\nTokenization is necessary for preparing data for embedding layer (see model architecture section below)'),
(6, 'Embedding layer', 'An embedding layer is a key layer to any sort of deep learning model that seeks ', 'An embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.'),
(7, 'An embedding layer ning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.', 'respond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.', 'An embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.'),
(8, 'Mamama', 'Hello you are on my site here', 'An embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.\r\nAn embedding layer is a key layer to any sort of deep learning model that seeks to understand words. What an embedding layer does from a mathematical standpoint is take a vector from a higher dimensional space (tens of thousands or more, the original size of our vocab) to a lower dimensional space (the amount of vectors we want to represent our data in, typically 100–300 in models like Word2Vec, Fasttext).\r\nHowever, it does this in such a way that words with similar meanings have similar mathematical values and exist in spaces that correspond to their meaning. Mathematical operations can be performed on these vectors, for example, ‘king’ minus ‘man’ may equal ‘royalty’.'),
(9, 'Hahaha', 'Stacked LSTMs may add more depth than additional cells in a single LSTM layer ', 'Stacked LSTMs may add more depth than additional cells in a single LSTM layer according to these folks at Cornell when applied to speech recognition. While our application is not identical, it is similar in its use of LSTM’s to try to identify language patterns, so we will try this architecture.\r\nThe first LSTM layer must have return sequences flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\r\nStacked LSTMs may add more depth than additional cells in a single LSTM layer according to these folks at Cornell when applied to speech recognition. While our application is not identical, it is similar in its use of LSTM’s to try to identify language patterns, so we will try this architecture.\r\nThe first LSTM layer must have return sequences flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\r\nStacked LSTMs may add more depth than additional cells in a single LSTM layer according to these folks at Cornell when applied to speech recognition. While our application is not identical, it is similar in its use of LSTM’s to try to identify language patterns, so we will try this architecture.\r\nThe first LSTM layer must have return sequences flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\r\n'),
(10, 'Hey you', 'Hello on this site', 'Stacked LSTMs may add more depth than additional cells in a single LSTM layer according to these folks at Cornell when applied to speech recognition. While our application is not identical, it is similar in its use of LSTM’s to try to identify language patterns, so we will try this architecture.\r\nThe first LSTM layer must have return sequences flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\r\nStacked LSTMs may add more depth than additional cells in a single LSTM layer according to these folks at Cornell when applied to speech recognition. While our application is not identical, it is similar in its use of LSTM’s to try to identify language patterns, so we will try this architecture.\r\nThe first LSTM layer must have return sequences flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\r\nStacked LSTMs may add more depth than additional cells in a single LSTM layer according to these folks at Cornell when applied to speech recognition. While our application is not identical, it is similar in its use of LSTM’s to try to identify language patterns, so we will try this architecture.\r\nThe first LSTM layer must have return sequences flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states'),
(11, 'Belarusian Presidential Election 2020: The Game is On', 'According to Lidzyia Yarmoshyna, head of the central electoral commission, the presidential election of 2020 in Belarus is to be held on August 30. Although many believed that the authorities will try to conduct the election in 2019 or the early beginning of 2020, the next election is to take place on the last day specified by the electoral law. Interestingly, the election day coincides with the birthday of the incumbent president.', 'Who are the candidates?\r\nDuring all the previous presidential elections in Belarus, there were attempts to find a united opposition candidate as a single opponent to Lukashenka. This goal was more or less achieved in 2006 when opposition chose Aliaksandr Milinkevich. Later, however, another candidate – Aliaksandr Kazulin – came to the forefront dividing the votes from the opposition.\r\nToday, chances for a single opposition candidate are even smaller. As soon as Yarmoshyna named the August 30, 2020 as a potential day for the election, several opposition forces expressed their intention to run as opponents to Lukashenka (who himself already publicly spoke about running for the presidency).\r\n\r\nAlena Anisim, who is currently one of the two opposition parliamentarians and the head of the organization Belarusian Language Society, said that she hopes to be able to unite a large part of the population.\r\n\r\nYury Hubarevich, the head of the Movement for Freedom, Mikalai Kazlou, the representative of the United Civic Party and Paval Seviarynets, co-chairman of the Belarusian Christian Democracy, announced that they will participate in the primaries. This will determine the candidate for the center-right coalition.\r\n\r\nMikalai Statkevich, the head of the People’s Hramada and one of the main opponents of Lukashenka in 2010, was put forward by the Belarusian National Congress. The Liberal Democratic party’s representative, Aleh Haidukevich, has expressed his intention to run for presidency highlighting economy and interests of business. Belarusian People’s Party has put forward the deputy chairman of their party, Aliaksei Yanukevich. There is also former political prisoner Siarhei Skrabets.\r\nWe need to change this. Vive Belarus'),
(12, 'Focus of the campaign', 'The upcoming presidential campaign is likely to focus on independence and potential Russian invasion. It was widely discussed and speculated upon in the last few years due to the Russian aggression in Ukraine. In the same line, the campaign will concentrate on the issue of cooperation with the EU.', 'The campaign will touch upon the the economics issues like potential economic models from the opposition candidates, proposals on developing the private sector and improving conditions for smaller businesses. The regime will traditionally stick to its arguments involving stability, continued state ownership of state enterprises and potential increase in salaries and pensions.\r\n\r\nWill this election be different?\r\nMany believe that the presidential election 2020 might become the last term of Lukashenka considering his earlier suggestion of strengthening the role of the parliament and government.\r\n\r\n‘This way Lukashenka is preparing for the transition of power. He has already stated that he does not want to leave as much authority to the next president as the current Constitution gives him,’ journalist Artsiom Shraibman writes for carnegie.ru.\r\nFor now, there are no expectations that the election will be free and fair, as the routine of election fraud in Belarus was evident at every election.\r\n\r\nOpposition politicians in Belarus do not participate in the political struggle. They rather play roles in the production show, whose script is written by the Lukashenka’s regime. There is no politics or elections in the country – although many want to play their simulation. As a result, we all live in the Performance Society,” writes philosopher Uladzimir Matskevich.\r\n\r\nOne more peculiarity of the election 2020 is the budget that has significantly increased compared to all previous elections. The state budget for 2020 shows that the regime plans to spend $18,1 mln organizing the election, which is $3m more than was spent on the presidential election 2015, notes Officelife.\r\n\r\n');

--
-- Индексы сохранённых таблиц
--

--
-- Индексы таблицы `news`
--
ALTER TABLE `news`
  ADD PRIMARY KEY (`id`);

--
-- AUTO_INCREMENT для сохранённых таблиц
--

--
-- AUTO_INCREMENT для таблицы `news`
--
ALTER TABLE `news`
  MODIFY `id` int(11) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=13;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
